Reproduction Instructions for VLDB Reviewers
============================================

Machine Requirements
--------------------
Our hundred-million scale experiments are run on a [Standard_E96_v5 Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/ev5-esv5-series#ev5-series) virtual machine with a 2 TB SSD. Our billion-scale experiments are run on a [Standard_M192is_v2](https://learn.microsoft.com/en-us/azure/virtual-machines/msv2-mdsv2-series) Azure virtual machine with a 20 TB SSD. These instructions will assume you are running experiments on a machine large enough to work on the billion size datasets, but the scripts can be easily modified to restrict to smaller datasets.

Out of these storage requirements, 1 TB is strictly necessary for the 100 million size datasets, and 2 TB is necessary for the billion size datasets. The remaining requirements are for storing the ANNS graphs generated by the experiments. We recommend storing the graphs so that they can be re-queried without rebuilding the graph if needed, but if external memory is a concern it can be foregone. 

Prerequisites
-------------
Clone [pbbsbench](https://github.com/cmuparlay/pbbsbench-vldb2024) and [big-ann-benchmarks](https://github.com/harsha-simhadri/big-ann-benchmarks) into your home directory.
For big-ann-benchmarks, use [these instructions](https://github.com/harsha-simhadri/big-ann-benchmarks) for setup. 
For pbbsbench, use [these instructions](https://cmuparlay.github.io/pbbsbench/) for setup.
You need an SSD mounted; the scripts assume it is named /ssd1. You should create the following directories: /ssd1/data, /ssd1/results, /ssd1/results/bigann, /ssd1/results/MSSPACEV1B, /ssd1/results/text2image1B, /ssd1/results/FB_ssnpp. Then, create a symlink named data to /ssd1/data in ~/big-ann-benchmarks. 

Data Preparation
----------------
To prepare the datasets, run "download_datasets.sh." It may need to be re-run two or three times as the download connection occasionally fails. Then, run "prepare_datasets.sh," which will compute a few missing groundtruth files. 

Experiments
-----------

To make sure everything is working correctly, we recommend first running "all_1M.sh." This will run a set of million-size experiments and write the results as CSV files to /ssd1/results. 

Next, the script "all_experiments.sh" will run every experiment in our paper. On the Msv2 machine, it took around 15 days to finish every experiment. This time can be curtailed to 2-3 days if needed by running experiments only on 100 million size datasets. 

